<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Travis Green</title>
    <link>https://greent3.github.io/tags/python/</link>
    <description>Recent content in Python on Travis Green</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://greent3.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>B. Django E-Learning Platform</title>
      <link>https://greent3.github.io/projects/creations/2_e-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/2_e-learning/</guid>
      <description>Coming Soon!</description>
    </item>
    
    <item>
      <title>C. TensorFlow Road Sign Classifier</title>
      <link>https://greent3.github.io/projects/creations/3_road_sign_classifier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/3_road_sign_classifier/</guid>
      <description>As research continues in the development of self-driving cars, one of the key challenges is allowing these cars to develop an understanding of their environment from digital images.
In my traffic-sign classification project, I used TensorFlow and the German Traffic Sign Recognition Benchmark dataset to build a neural network that puts images of road signs into different classifications.
My role consisted of loading the data (images) into the program, resizing the images, and designing/implementing the neural network.</description>
    </item>
    
    <item>
      <title>D. RESTful Movie Database API w/ extensive unit testing</title>
      <link>https://greent3.github.io/projects/creations/4_restful-imdb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/4_restful-imdb/</guid>
      <description>The project is divided into 2 apps. Movielist app: handles all movie database functionality (adding movies to the database, querying lists of movies, creating/viewing reviews, etc.) along with their associated permissions. User app: handles user functionality (login, register, etc.) The unit testing covers all intented functionality for each level of authentication. The project also employs validators, pagination, and throttling.</description>
    </item>
    
    <item>
      <title>G. Ecommerce Purchase Predictor</title>
      <link>https://greent3.github.io/projects/creations/7_knn_purchase_predictor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/7_knn_purchase_predictor/</guid>
      <description>When we run our program, we pass the location of a CSV file containing data about website shoppers as a command line argument. Our program:
loads in that data reformats it into the correct data type splits it into training and testing data uses that data to train a k-nearest neighbors classification model and outputs the sensitivity (true positive rate) and specificity (true negative rate) of our trained model on our testing data.</description>
    </item>
    
    <item>
      <title>H. Whisky Review Sentiment Classifier</title>
      <link>https://greent3.github.io/projects/creations/8_naive_bayes_review_classifier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/8_naive_bayes_review_classifier/</guid>
      <description>Classifying Japanese Whisky reviews using TFIDF/Naive Bayes
Given a CSV file with 200 Japanese whisky reviews labeled &amp;ldquo;positive&amp;rdquo; or &amp;ldquo;negative&amp;rdquo;, and 900 unlabeled reviews, let&amp;rsquo;s train a Naive Bayes model on TFIDF data so we can predict the labels for the remaining 900.
Note: The dataset was not that great (small, lots of typos, many negative reviews with positive sentiment words &amp;ldquo;ex: not that good&amp;rdquo;), but it still served as good practice for cleaning data, NB, vectorizers, working with dataframes, and even a little bit of SQL at the end.</description>
    </item>
    
    <item>
      <title>I. Car Price Predictor</title>
      <link>https://greent3.github.io/projects/creations/9_car_price_predictor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/9_car_price_predictor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>J. Golden Cross Trading Algo (video)</title>
      <link>https://greent3.github.io/projects/creations/93_golden_cross/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/93_golden_cross/</guid>
      <description>Buys or sells an equity in anticipation of the 5 period SMA crossing the 20 period SMA and employs a trailing stoploss. Resolution, SMA periods, stoplosses, and order entry/exit points are all optimizable.
Video linked below!</description>
    </item>
    
    <item>
      <title>L. Detective Conan Web Scraper</title>
      <link>https://greent3.github.io/projects/creations/95_scraping_detective_conan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/95_scraping_detective_conan/</guid>
      <description>Yay! We convinced our buddy (Doug) to watch Detective Conan! However, he already read the manga and doesn&amp;rsquo;t want to watch episodes that he&amp;rsquo;s already read. No worries! Let&amp;rsquo;s use the data at Detective Conan Wiki to compile a list of episodes that weren&amp;rsquo;t in the manga.</description>
    </item>
    
    <item>
      <title>M. Teaching a robot NIM</title>
      <link>https://greent3.github.io/projects/creations/96_nim_bot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://greent3.github.io/projects/creations/96_nim_bot/</guid>
      <description>By playing against itself repeatedly and learning from experience, eventually our AI will learn which actions to take and which actions to avoid. We do this through Q-learning, or assigning a reward value for every (state, action) pair. Weâ€™ll represent the game as an array (game board) of numbers (# of pieces in each row).</description>
    </item>
    
  </channel>
</rss>
